{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafc0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from img2net import *\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from graphinference import *\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e27eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#largest connected component\n",
    "def graph_metrics(G):\n",
    "    # calculate the normalized average degree\n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    #avg_degree = sum(degrees) / len(degrees)\n",
    "    \n",
    "    # Get the number of components\n",
    "    num_components = nx.number_connected_components(G)\n",
    "\n",
    "    #print(\"Number of components:\", num_components)\n",
    "    \n",
    "    # Perform hierarchical clustering using Girvan-Newman algorithm\n",
    "    clusters = nx.algorithms.community.girvan_newman(G)\n",
    "\n",
    "    # Get the number of clusters\n",
    "    num_clusters = len(list(clusters))\n",
    "\n",
    "    #print(\"Number of clusters:\", num_clusters)\n",
    "    \n",
    "    # Calculate the biggest component\n",
    "    biggest_component = max(nx.connected_components(G), key=len)\n",
    "    \n",
    "    # Create a subgraph of the biggest component\n",
    "    subgraph = G.subgraph(biggest_component)\n",
    "    \n",
    "        # Compute eccentricity for all nodes\n",
    "    eccentricities = nx.eccentricity(subgraph)\n",
    "\n",
    "    # Print the eccentricity of each node\n",
    "    #for node, eccentricity in eccentricities.items():\n",
    "        #print(f\"Eccentricity of node {node}: {eccentricity}\")\n",
    "    \n",
    "    # Find the shortest path length\n",
    "    shortest_path_length = nx.average_shortest_path_length(subgraph)\n",
    "\n",
    "\n",
    "    avg_cc = nx.average_clustering(subgraph)\n",
    "\n",
    "    \n",
    "    return degrees, num_components, num_clusters, eccentricities,  avg_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d7f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pixel_to_um(pixel_measurement):\n",
    "    # calculate the micrometer measurement\n",
    "    scale_factor=5.86 #m\n",
    "    magnification=5\n",
    "    micrometer_measurement = pixel_measurement*(( scale_factor / magnification) ** 2)\n",
    "    return micrometer_measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f5fc4",
   "metadata": {},
   "source": [
    "# Save adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d28ff582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022_08_15_D16_D18_CS6.tif', '2022_08_15_D16_D18_CS3.tif', '2022_08_09_D12_CS2_MAX_Rec_BS.tif', '2022_08_11_D08_CS5_5X.tif', '2022_10_07_D16_D15_CS3.tif', '2022_08_19_D08_CS5_5X.tif', '2022_06_28_D12_CS1_7_5X.tif', '2022_05_30_D16_D18_CS1.tif', '2022_08_19_D08_CS1_5X.tif', '2022_08_19_D08_CS3_5X.tif', '2022_08_15_D16_D18_CS5_MAX_Rec_BS.tif', '2022_05_30_D16_D18_CS5.tif', '2022_08_11_D08_CS3_5X.tif', '2022_08_09_D12_CS6_MAX_Rec_BS.tif', '2022_08_15_D16_D18_CS2_MAX_Rec_BS.tif', '2022_10_07_D16_D15_CS5_MAX_Rec_BS.tif', '2023_03_28_D12_CS5_R3_MAX_Rec_BS.tif', '2023_03_28_D12_CS3_R1_MAX_Rec_BS.tif', '2023_03_28_D12_CS1_R2_MAX_Rec_BS.tif', '2022_08_11_D08_CS6_5X.tif', '2022_08_19_D08_CS4_5X.tif', '2023_03_28_D12_CS4_R3_MAX_Rec_BS.tif', '2023_03_28_D12_CS2_R1_MAX_Rec_BS.tif', '2022_08_11_D08_CS4_5X.tif', '2022_11_28_D16_D18_CS2_6f.tif', '2023_03_28_D12_CS1_R3_MAX_Rec_BS.tif', '2022_10_07_D16_D15_CS4_MAX_Rec_BS.tif', '2023_03_28_D12_CS3_R2_MAX_Rec_BS.tif', '2022_10_07_D16_D15_CS1_MAX_Rec_BS.tif', '2022_05_27_D08_5X.tif', '2023_03_28_D12_CS2_R2_MAX_Rec_BS.tif', '2022_08_11_D08_CS2_5X.tif', '2022_08_19_D08_CS2_5X.tif']\n",
      "2022_08_15_D16_D18_CS6 0\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "2022_08_15_D16_D18_CS3 1\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "2022_08_09_D12_CS2_MAX_Rec_BS 2\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "2022_08_11_D08_CS5_5X 3\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "2022_10_07_D16_D15_CS3 4\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_08_19_D08_CS5_5X 5\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_06_28_D12_CS1_7_5X 6\n",
      "64/64 [==============================] - 3s 44ms/step\n",
      "64/64 [==============================] - 3s 44ms/step\n",
      "2022_05_30_D16_D18_CS1 7\n",
      "64/64 [==============================] - 3s 44ms/step\n",
      "64/64 [==============================] - 3s 44ms/step\n",
      "2022_08_19_D08_CS1_5X 8\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_08_19_D08_CS3_5X 9\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 43ms/step\n",
      "2022_08_15_D16_D18_CS5_MAX_Rec_BS 10\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_05_30_D16_D18_CS5 11\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_08_11_D08_CS3_5X 12\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "2022_08_09_D12_CS6_MAX_Rec_BS 13\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "2022_08_15_D16_D18_CS2_MAX_Rec_BS 14\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_10_07_D16_D15_CS5_MAX_Rec_BS 15\n",
      "64/64 [==============================] - 3s 45ms/step\n",
      "64/64 [==============================] - 3s 46ms/step\n",
      "2023_03_28_D12_CS5_R3_MAX_Rec_BS 16\n",
      "64/64 [==============================] - 3s 44ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2023_03_28_D12_CS3_R1_MAX_Rec_BS 17\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "2023_03_28_D12_CS1_R2_MAX_Rec_BS 18\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_08_11_D08_CS6_5X 19\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "2022_08_19_D08_CS4_5X 20\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2023_03_28_D12_CS4_R3_MAX_Rec_BS 21\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2023_03_28_D12_CS2_R1_MAX_Rec_BS 22\n",
      "64/64 [==============================] - 3s 42ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "2022_08_11_D08_CS4_5X 23\n",
      "64/64 [==============================] - 3s 48ms/step\n",
      "64/64 [==============================] - 3s 46ms/step\n",
      "2022_11_28_D16_D18_CS2_6f 24\n",
      "64/64 [==============================] - 3s 46ms/step\n",
      "64/64 [==============================] - 3s 45ms/step\n",
      "2023_03_28_D12_CS1_R3_MAX_Rec_BS 25\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "2022_10_07_D16_D15_CS4_MAX_Rec_BS 26\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2023_03_28_D12_CS3_R2_MAX_Rec_BS 27\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 43ms/step\n",
      "2022_10_07_D16_D15_CS1_MAX_Rec_BS 28\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "2022_05_27_D08_5X 29\n",
      "64/64 [==============================] - 3s 42ms/step\n",
      "64/64 [==============================] - 3s 42ms/step\n",
      "2023_03_28_D12_CS2_R2_MAX_Rec_BS 30\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "2022_08_11_D08_CS2_5X 31\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "2022_08_19_D08_CS2_5X 32\n",
      "64/64 [==============================] - 3s 46ms/step\n",
      "64/64 [==============================] - 3s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_path = \"/Users/behnaz/NCADD/prediction_cell_neurite/data/3SUBDIV\"#/sub/sub0\"\n",
    "cell_model_path = \"DHM_Cell.hdf5\"\n",
    "neurite_model_path = \"DHM_Neurite.hdf5\"\n",
    "patchsize = 128\n",
    "overlap = 0.2\n",
    "# Get a list of all .tif files in the test path\n",
    "file_list = [f for f in os.listdir(test_path) if f[-3:] == \"tif\"]\n",
    "print(file_list)\n",
    "\n",
    "# Calculate graph metrics and append the values to the arrays for each image\n",
    "for i in range(len(file_list)):\n",
    "    \n",
    "    # Extract the ID from the file name and append the file to the appropriate group\n",
    "    id = file_list[i][:-4]\n",
    "    print(id,i)\n",
    "    \n",
    "    # Read the image\n",
    "    img = img_read(test_path, file_list[i])\n",
    "    \n",
    "    cell_mask, neurite_mask, cell_num = predict_and_segment(img, cell_model_path, neurite_model_path, patchsize, overlap)\n",
    "   \n",
    "    vertices, weights, paths, paths_matrix = infer_structural_graph(cell_mask, neurite_mask, find_paths=True)\n",
    "    adj_mat = weights > 0\n",
    "\n",
    "    # Save adj_mat as an npy file with the name of the current file_list item\n",
    "    np.save(f\"{id}.npy\", adj_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c0fc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022_08_15_D16_D18_CS6.tif', '2022_08_15_D16_D18_CS3.tif', '2022_08_09_D12_CS2_MAX_Rec_BS.tif', '2022_08_11_D08_CS5_5X.tif', '2022_10_07_D16_D15_CS3.tif', '2022_08_19_D08_CS5_5X.tif', '2022_06_28_D12_CS1_7_5X.tif', '2022_05_30_D16_D18_CS1.tif', '2022_08_19_D08_CS1_5X.tif', '2022_08_19_D08_CS3_5X.tif', '2022_08_15_D16_D18_CS5_MAX_Rec_BS.tif', '2022_05_30_D16_D18_CS5.tif', '2022_08_11_D08_CS3_5X.tif', '2022_08_09_D12_CS6_MAX_Rec_BS.tif', '2022_08_15_D16_D18_CS2_MAX_Rec_BS.tif', '2022_10_07_D16_D15_CS5_MAX_Rec_BS.tif', '2023_03_28_D12_CS5_R3_MAX_Rec_BS.tif', '2023_03_28_D12_CS3_R1_MAX_Rec_BS.tif', '2023_03_28_D12_CS1_R2_MAX_Rec_BS.tif', '2022_08_11_D08_CS6_5X.tif', '2022_08_19_D08_CS4_5X.tif', '2023_03_28_D12_CS4_R3_MAX_Rec_BS.tif', '2023_03_28_D12_CS2_R1_MAX_Rec_BS.tif', '2022_08_11_D08_CS4_5X.tif', '2022_11_28_D16_D18_CS2_6f.tif', '2023_03_28_D12_CS1_R3_MAX_Rec_BS.tif', '2022_10_07_D16_D15_CS4_MAX_Rec_BS.tif', '2023_03_28_D12_CS3_R2_MAX_Rec_BS.tif', '2022_10_07_D16_D15_CS1_MAX_Rec_BS.tif', '2022_05_27_D08_5X.tif', '2023_03_28_D12_CS2_R2_MAX_Rec_BS.tif', '2022_08_11_D08_CS2_5X.tif', '2022_08_19_D08_CS2_5X.tif']\n",
      "D16 0\n",
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/behnaz/NCADD/prediction_cell_neurite/Prediction_pipeline_Cell+Neurite/img2net.py:99: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  c_results = cell_model.predict_generator(test_gene, num_test_images, verbose=1)\n",
      "2023-05-18 14:45:00.991398: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 3s 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/behnaz/NCADD/prediction_cell_neurite/Prediction_pipeline_Cell+Neurite/img2net.py:108: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  n_results = neurite_model.predict_generator(test_gene, num_test_images, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 3s 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0c/_vv7vm4n5z33nmfk83_ct2480000gn/T/ipykernel_2280/3482897934.py:54: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  neurite_length = np.reciprocal(weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/behnaz/NCADD/prediction_cell_neurite/Prediction_pipeline_Cell+Neurite/img2net.py:99: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  c_results = cell_model.predict_generator(test_gene, num_test_images, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 3s 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/behnaz/NCADD/prediction_cell_neurite/Prediction_pipeline_Cell+Neurite/img2net.py:108: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  n_results = neurite_model.predict_generator(test_gene, num_test_images, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 2\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 3\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 37ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 4\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 5\n",
      "64/64 [==============================] - 3s 37ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 6\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 7\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 8\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 9\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 10\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 11\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 12\n",
      "64/64 [==============================] - 3s 37ms/step\n",
      "64/64 [==============================] - 3s 37ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 13\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 14\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 15\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 16\n",
      "64/64 [==============================] - 3s 37ms/step\n",
      "64/64 [==============================] - 3s 37ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 17\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 18\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 19\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 20\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 21\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 22\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 23\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 24\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 25\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 26\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 27\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 38ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D16 28\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 29\n",
      "64/64 [==============================] - 3s 40ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D12 30\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 31\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n",
      "D08 32\n",
      "64/64 [==============================] - 3s 41ms/step\n",
      "64/64 [==============================] - 3s 39ms/step\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "test_path = \"/Users/behnaz/NCADD/prediction_cell_neurite/data/3SUBDIV\"#/sub/sub0\"\n",
    "cell_model_path = \"DHM_Cell.hdf5\"\n",
    "neurite_model_path = \"DHM_Neurite.hdf5\"\n",
    "patchsize = 128\n",
    "overlap = 0.2\n",
    "\n",
    "# Create arrays to store the calculated values\n",
    "degree_arr = []\n",
    "num_components_arr = []\n",
    "num_clusters_arr = []\n",
    "eccentricities_arr = []\n",
    "avg_cc_arr = []\n",
    "cell_watershed = []\n",
    "cell_areas = []\n",
    "group_data_neurite_length = {}\n",
    "\n",
    "\n",
    "# Create a dictionary to group the images based on their ID\n",
    "groups = {'D08':[],'D12':[],'D16':[]}\n",
    "\n",
    "# Get a list of all .tif files in the test path\n",
    "file_list = [f for f in os.listdir(test_path) if f[-3:] == \"tif\"]\n",
    "print(file_list)\n",
    "\n",
    "# Calculate graph metrics and append the values to the arrays for each image\n",
    "for i in range(len(file_list)):\n",
    "    \n",
    "    # Extract the ID from the file name and append the file to the appropriate group\n",
    "    id = file_list[i].split('_')[3]\n",
    "    print(id,i)\n",
    "    if id in groups:\n",
    "        groups[id].append(file_list[i])\n",
    "    # Read the image\n",
    "    img = img_read(test_path, file_list[i])\n",
    "    \n",
    "    cell_mask, neurite_mask, cell_num = predict_and_segment(img, cell_model_path, neurite_model_path, patchsize, overlap)\n",
    "    \n",
    "    cell_watershed.append(cell_num)\n",
    "    \n",
    "    # Compute regions from binary mask\n",
    "    regions = measure.regionprops(label_image=cell_mask)\n",
    "    cell_areas.append([convert_pixel_to_um(region.area) for region in regions])\n",
    "    \n",
    "\n",
    "    vertices, weights, paths, paths_matrix = infer_structural_graph(cell_mask, neurite_mask, find_paths = True)\n",
    "    adj_mat = weights>0\n",
    "    G = nx.Graph(adj_mat)\n",
    "    #compute neurite length\n",
    "    neurite_length = np.reciprocal(weights)\n",
    "    neurite_length[np.isinf(neurite_length)] = 0\n",
    "    up_neurite_length = np.triu(neurite_length,1)\n",
    "    non_zero_mask = up_neurite_length != 0\n",
    "    neurite_length = up_neurite_length[non_zero_mask]\n",
    "    print(type(neurite_length))\n",
    "    # Append the neurite length to the group data dictionary\n",
    "    if id in group_data_neurite_length:\n",
    "        group_data_neurite_length[id].extend(neurite_length)\n",
    "    else:\n",
    "        group_data_neurite_length[id] = list(neurite_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate graph metrics for the current image\n",
    "    degrees, num_components, num_clusters, eccentricities,  avg_cc = graph_metrics(G)\n",
    "    print(type(eccentricities))\n",
    "    avg_cc_arr.append(avg_cc)\n",
    "    # Append the calculated values to the arrays\n",
    "    degree_arr.append(degrees)\n",
    "    num_components_arr.append(num_components)\n",
    "    num_clusters_arr.append(num_clusters)\n",
    "    # Save eccentricity values in the dictionary\n",
    "    eccentricities_arr.append(eccentricities)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b857b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['D08', 'D12', 'D16'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a dictionary to store the arrays for each group\n",
    "group_data_eccentricities_arr = {group: [] for group in groups}\n",
    "\n",
    "\n",
    "\n",
    "# Iterate through the files and append the corresponding array value to the group data\n",
    "for i in range(len(eccentricities_arr)):\n",
    "    file = file_list[i]\n",
    "    id = file.split('_')[3]\n",
    "    group_data_eccentricities_arr[id].append(eccentricities_arr[i])\n",
    "group_data_eccentricities_arr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9f035db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('group_data_eccentricities_arr.json', 'w') as file:\n",
    "    json.dump(group_data_eccentricities_arr, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dca41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree_arr = []\n",
    "# num_components_arr = []\n",
    "# num_clusters_arr = []\n",
    "# avg_cc_arr = []\n",
    "# cell_watershed = []\n",
    "# cell_areas = []\n",
    "# group_data_neurite_length = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf5241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a dictionary to store the arrays for each group\n",
    "group_data_degree_arr = {group: [] for group in groups}\n",
    "group_data_num_components_arr = {group: [] for group in groups}\n",
    "group_data_num_clusters_arr = {group: [] for group in groups}\n",
    "group_data_avg_cc_arr = {group: [] for group in groups}\n",
    "group_data_cell_watershed = {group: [] for group in groups}\n",
    "group_data_cell_areas = {group: [] for group in groups}\n",
    "\n",
    "\n",
    "# Iterate through the files and append the corresponding array value to the group data\n",
    "for i in range(len(degree_arr)):\n",
    "    file = file_list[i]\n",
    "    id = file.split('_')[3]\n",
    "    group_data_degree_arr[id].append(degree_arr[i])\n",
    "    group_data_num_components_arr[id].append(num_components_arr[i])\n",
    "    group_data_num_clusters_arr[id].append(num_clusters_arr[i])\n",
    "    group_data_avg_cc_arr[id].append(avg_cc_arr[i])\n",
    "    group_data_cell_watershed[id].append(cell_watershed[i])\n",
    "    group_data_cell_areas[id].append(cell_areas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9afab",
   "metadata": {},
   "source": [
    "# Saving dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6dbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Save the dictionary to a file\n",
    "with open('group_data_neurite_length.json', 'w') as file:\n",
    "    json.dump(group_data_neurite_length, file)\n",
    "    \n",
    "with open('group_data_degree_arr.json', 'w') as file:\n",
    "    json.dump(group_data_degree_arr, file)\n",
    "    \n",
    "with open('group_data_num_components_arr.json', 'w') as file:\n",
    "    json.dump(group_data_num_components_arr, file)\n",
    "    \n",
    "with open('group_data_num_clusters_arr.json', 'w') as file:\n",
    "    json.dump(group_data_num_clusters_arr, file)   \n",
    "    \n",
    "with open('group_data_avg_cc_arr.json', 'w') as file:\n",
    "    json.dump(group_data_avg_cc_arr, file)   \n",
    "    \n",
    "# with open('group_data_cell_watershed.json', 'w') as file:\n",
    "#     json.dump(group_data_cell_watershed, file)   \n",
    "    \n",
    "with open('group_data_cell_areas.json', 'w') as file:\n",
    "    json.dump(group_data_cell_areas, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6baa3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Convert the NumPy data types to Python data types\n",
    "def convert_np_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        raise TypeError(f\"Object of type {obj.__class__.__name__} is not JSON serializable\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the dictionary to a file\n",
    "with open('group_data_cell_watershed.json', 'w') as file:\n",
    "    json.dump(group_data_cell_watershed, file, default=convert_np_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
